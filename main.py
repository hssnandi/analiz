import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import cv2
import numpy as np
import mediapipe as mp
import os
from PIL import Image
from face_shape_detector import detect_face_shape
from utils import analyze_image_quality, draw_overlay

st.set_page_config(page_title="ØªØ´Ø®ÛŒØµ ÙØ±Ù… ØµÙˆØ±Øª", layout="centered")
st.title("ğŸ“¸ ØªØ´Ø®ÛŒØµ ÙØ±Ù… ØµÙˆØ±Øª Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ø¯Ù„ Ù…Ùˆ")

st.markdown("""
<div style='text-align:center;'>
    Ù„Ø·ÙØ§Ù‹ ØµÙˆØ±Øª Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ù…Ø±Ú©Ø² Ø¯Ø§ÛŒØ±Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯. Ù†ÙˆØ± Ú©Ø§ÙÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯. Ø§Ø² Ø¹ÛŒÙ†Ú© ÛŒØ§ Ù…Ø§Ø³Ú© Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.
</div>
""", unsafe_allow_html=True)

captured_image = None
status_placeholder = st.empty()
captured_img_placeholder = st.empty()

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)

image_holder = {"image": None}

def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(img_rgb)

    overlay = draw_overlay(img_rgb.copy())
    quality_ok, reason = analyze_image_quality(results, img_rgb)

    center_color = (0, 255, 0) if quality_ok else (128, 128, 128)
    cv2.circle(overlay, (320, 240), 140, center_color, 3)

    if not quality_ok:
        status_placeholder.warning(reason)
    else:
        status_placeholder.info("âœ… Ø´Ø±Ø§ÛŒØ· Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³ØªØŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø¹Ú©Ø³ Ø±ÙˆÛŒ Ø¯Ú©Ù…Ù‡ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯.")

    image_holder["image"] = img_rgb if quality_ok else None
    return av.VideoFrame.from_ndarray(overlay, format="rgb24")

webrtc_ctx = webrtc_streamer(
    key="face-analyzer",
    mode=WebRtcMode.SENDRECV,
    video_frame_callback=video_frame_callback,
    media_stream_constraints={"video": True, "audio": False},
    async_processing=True,
)

if st.button("ğŸ“· Ú¯Ø±ÙØªÙ† Ø¹Ú©Ø³"):
    if image_holder["image"] is not None:
        captured_image = image_holder["image"]
        captured_img_placeholder.image(captured_image, caption="ØªØµÙˆÛŒØ± Ø«Ø¨Øª Ø´Ø¯Ù‡", use_column_width=True)
    else:
        st.error("â›” Ø´Ø±Ø§ÛŒØ· Ø¹Ú©Ø³ Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ù†ÙˆØ±ØŒ Ù…ÙˆÙ‚Ø¹ÛŒØª ØµÙˆØ±Øª Ùˆ Ù…Ø±Ú©Ø² ØªØµÙˆÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")

if captured_image is not None:
    st.subheader("ğŸ“Š Ø¢Ù†Ø§Ù„ÛŒØ² ÙØ±Ù… ØµÙˆØ±Øª")
    shape = detect_face_shape(captured_image)
    st.success(f"ÙØ±Ù… ØµÙˆØ±Øª Ø´Ù…Ø§: {shape}")

    st.subheader("ğŸ’‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ")
    model_path = os.path.join("model_images", shape.lower())
    if os.path.exists(model_path):
        imgs = os.listdir(model_path)
        for img in imgs:
            st.image(os.path.join(model_path, img), use_column_width=True)
    else:
        st.info("ØªØµØ§ÙˆÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙØ±Ù… ØµÙˆØ±Øª Ù‡Ù†ÙˆØ² Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.")
